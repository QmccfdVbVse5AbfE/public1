# -*- coding: utf-8 -*-
"""final_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r9zX_ATm0X3PdldbpNJydcRt3zraCc2k
"""

# Install packages
!pip install -U tmtoolkit
!pip install matplotlib==3.1.1
!pip install svgpath2mpl

from collections import defaultdict
import json
import pandas as pd 
import numpy as np 
import requests 
import datetime
from bs4 import BeautifulSoup
import statsmodels.api as sm
from statsmodels.tools import eval_measures
import copy

# Text processing imports
import spacy
parser = spacy.load("en_core_web_sm")
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from tmtoolkit.topicmod.evaluate import metric_coherence_gensim

# Visualization imports
import seaborn as sns
import matplotlib.pyplot as plt
from svgpath2mpl import parse_path

# Statistical test imports
import scipy
import statsmodels
from scipy.stats import friedmanchisquare
from statsmodels.graphics.gofplots import qqplot

"""# DATA DELIVERABLE (CHECKPOINT 1)

## CONSTRUCT THE DATASET
"""

# Mount Google Drive locally
from google.colab import drive
drive.mount('/content/drive')

# Dataset from: https://www.whitehouse.gov/omb/historical-tables/
# NOTE: make sure the shared projet folder follows this path
with open('/content/drive/My Drive/CS1951A Final Project/hist05z2_fy22.csv', 'r', encoding='utf-8') as data:
  budget_df = pd.read_csv(data, header=2, index_col="Department or other unit")

budget_df.head(4)

# Webscrape State of The Union Addresses
# Dataset from: https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union

speech_table_url = "https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union"
def get_yearly_speeches():
    response = requests.get(speech_table_url, auth=('user', 'pass'))
    print("response: ", response, type(response))
    print("response.text: ", response.text, type(response.text))
    html_dump = BeautifulSoup(response.text, 'html.parser')

    # list of all the things with the lable "table"
    html_tables = html_dump.find('table')
    cols = html_tables.find("tbody")
    all_speeches_url = []
    speeches = cols.find_all("tr")
    # print(stocks)
    for i in speeches:
        entry = i.find_all("td")
        for j in entry:
          potential_link = j.a
          # print(potential_link)
          if type(j.text.strip()) is str and len(j.text.strip()) > 9:
            president = j.text.strip().lower()
            # print("name = ",j.text.strip().lower())
          elif potential_link is not None:
            year = potential_link.text
            url = potential_link['href']
            # print(potential_link.text, potential_link['href'])
            all_speeches_url.append({"president": president, "year":year, "url": url})
    return all_speeches_url
all_speeches_url = get_yearly_speeches()
# print(all_speeches_url)

for speech in all_speeches_url:
  # print(speech['president'])
  if(speech['president'] == 'gerald r. ford'):
    break
  response = requests.get(speech['url'])
  html_dump = BeautifulSoup(response.text, 'html.parser')
  # cols = html_tables.find("tbody")
  stocks = html_dump.find_all("section")
  full_text_together = ""
  for i in stocks:
    full_text_broken = i.find_all("p")
    # print("text", full_text_broken)
    for j in full_text_broken:
      # print("text", full_text)
      full_text_together += j.text + " "
      speech['speech'] = (full_text_together)
# print(all_speeches_url)

# Convert to pandas DF
speeches_df = pd.DataFrame(all_speeches_url)
speeches_df['president'][0] = 'joe biden'
speeches_df.head(48)

"""Load in health dictionary dataset"""

with open('/content/drive/My Drive/CS1951A Final Project/health_dictionary.txt', 'r', encoding='utf-8') as data:
  items =  data.readlines()
  health_dic = [x.strip().lower() for x in items] 
print(health_dic)

health_dic.append(["insurance", "healthcare"])
print(len(health_dic))

"""# CLEAN AND PROCESS DATA"""

# Filter budget_df and transform strings to floats
health_budget_df = budget_df.loc[['Department of Health and Human Services', 'Total budget authority']]
health_budget_df.loc['Department of Health and Human Services'] = health_budget_df.loc['Department of Health and Human Services'].apply(lambda x: float(x.split()[0].replace(',', '')))
health_budget_df.loc['Total budget authority'] = health_budget_df.loc['Total budget authority'].apply(lambda x: float(x.split()[0].replace(',', '')))

# Transform budget_df into percentage (dept of health & human services / total budget authority)
percent_health_budget_series = pd.Series(health_budget_df.loc['Department of Health and Human Services'].divide(health_budget_df.loc['Total budget authority']), name='Percent health budget spend')
transformed_health_budget_df = health_budget_df.append(percent_health_budget_series)

# Drop columns past 2020 (speeches dataset until 2020) and TQ
transformed_health_budget_df = transformed_health_budget_df.drop(labels=['TQ', '2021 estimate', '2022 estimate', '2023 estimate', '2024 estimate', '2025 estimate', '2026 estimate'], axis=1)

transformed_health_budget_df

print(f"Raw budget df shape: {budget_df.shape}")
print(f"Health budget df shape: {transformed_health_budget_df.shape}")

print(f"Raw number of speeches: {speeches_df.shape}")
print(f"Raw number of Presidents: {speeches_df.groupby(by=['president'] , axis=0).count()['speech']}")

print(speeches_df[speeches_df['president'] == 'gerald r. ford'])

# Drop N/A rows in speeches data
speeches_df_filtered = speeches_df.dropna(axis=0, subset=['speech'])
# Removes "about search"
speeches_df_filtered["speech"] = speeches_df_filtered["speech"].apply(lambda x: x[14:])

# Filter out Joe Biden from dataframe (see reasoning in write-up)
speeches_df_filtered_with_biden = speeches_df_filtered.copy(deep=True)
speeches_df_filtered = speeches_df_filtered[speeches_df_filtered['president'] != 'joe biden']

print(f"Transformed number of speeches: {speeches_df_filtered.shape}")
print(f"Transformed number of Presidents: {speeches_df_filtered.groupby(by=['president'] , axis=0).count()['speech']}")

# Include partisanship for each president
conditions = [
              speeches_df_filtered['president'] == 'donald j. trump',
              speeches_df_filtered['president'] == 'barack obama',
              speeches_df_filtered['president'] == 'george w. bush',
              speeches_df_filtered['president'] == 'william j. clinton',
              speeches_df_filtered['president'] == 'george bush',
              speeches_df_filtered['president'] == 'ronald reagan',
              speeches_df_filtered['president'] == 'jimmy carter',
]

is_dem = ['False', 'True', 'False', 'True', 'False', 'False', 'True']
speeches_df_filtered['is_democrat'] = np.select(conditions, is_dem)

speeches_df_filtered.head()

# Convert to text speeches to series
speeches_series = speeches_df_filtered_with_biden['speech']
speeches_series.head(5)

# Customize tokenizer to remove '[' and ']'
suffixes = list(parser.Defaults.suffixes)
suffixes.remove("\]")
suffix_regex = spacy.util.compile_suffix_regex(suffixes)
parser.tokenizer.suffix_search = suffix_regex.search

prefixes = list(parser.Defaults.prefixes)
prefixes.remove("\[")
prefix_regex = spacy.util.compile_prefix_regex(prefixes)
parser.tokenizer.prefix_search = prefix_regex.search

# Run parser on all speeches to tokenize and create a list of spacy docs
parsed_speeches = []

for speech in speeches_series:    
  parsed_speeches.append(parser(speech))

extra_stop_words = ['america', 'american', 'americans', 'government', 'thank', 'year', 'nation', 'people', 'country']

# Split str of words into list and lowercase
normalized_speeches = []

for speech in parsed_speeches:
  new_speech = []
  for w in speech:
      # Filter out words in between [], stop words, and puncutation
      if (
          not w.is_stop and not w.is_punct and not w.is_space and w.is_alpha and 
          (w.text[0] != '[' and w.text[-1] != ']') and 
          (w.lemma_.lower() not in extra_stop_words)
      ):
        # Lowercase before appending
        new_speech.append(w.lemma_.lower())
  
  normalized_speeches.append(new_speech)

print(normalized_speeches[0])

speeches_df_filtered['speech'] = normalized_speeches[1:]
speeches_df_filtered_with_biden['speech'] = normalized_speeches

"""Join speech data and health budget data  
**For separate analyses, use speeches_df_filtered and transformed_health_budget_df**
"""

# Pivot health budget data to prepare for join
years = transformed_health_budget_df.columns.values
percentages = transformed_health_budget_df.iloc[2]
health_budget_dict = {years[i]: percentages[i] for i in range(len(years))}

pivoted_health_df = pd.DataFrame.from_dict(health_budget_dict, orient='index', columns=['percent health budget spend'])
pivoted_health_df.reset_index(inplace=True)
pivoted_health_df.rename(columns={'index':'year'}, inplace=True)
# pivoted_health_df.head()

# Perform merge
merged_df = pd.merge(speeches_df_filtered, pivoted_health_df, how='inner', on='year')
merged_df

"""# DEFINE TRAIN AND TEST SET

"""

from random import randrange

test_set_indices = []
president_array = np.unique(np.array(speeches_df_filtered['president']))
two_speeches = ['george bush', 'donald j. trump']

#selecting one speech from each president
for prez in president_array:
  prez_filtered = speeches_df_filtered[speeches_df_filtered["president"] == prez]
  speech_sampled = prez_filtered.sample()['speech']
  test_set_indices.append(speech_sampled.index[0])  # Subtract 1 b/c indices go from 1 to 47 (instead of 0 to 46)
  # test_speeches.append(prez_sampled[['president','speech','year']].values)

#getting randomly selected unique speeches
while len(test_set_indices) < 10:
  more_speeches = speeches_df_filtered[~speeches_df_filtered["president"].isin(two_speeches)]
  sample = more_speeches.sample()['speech']
  test_set_indices.append(int(sample.index[0]))

print(f"test set indices: {test_set_indices}")
test_set = merged_df.iloc[test_set_indices]
train_set = merged_df.drop(test_set_indices)
print(f"test set shape: {test_set.shape}")
print(f"train set shape: {train_set.shape}")

# FILE DOWNLOAD FOR CHECKPOINT 1
# from google.colab import files

# with open('complete_speeches_data.csv', 'w') as f:
#   f.write(speeches_df_filtered.to_csv())

# with open('test_data.csv', 'w') as f:
#   for i in test_speeches.keys():
#     f.write("%s,%s\n"%(i,test_speeches[i]))

# with open('complete_budget_data.csv', 'w') as f:
#   f.write(transformed_health_budget_df.to_csv())

# files.download('complete_speeches_data.csv')
# files.download('test_data.csv')
# files.download('complete_budget_data.csv')

"""# EXPLORATORY ANALYSIS

### Topic Modeling
[Can we extract a health-related dimension using topic modelling?]
"""

# Convert each speech from list of strings to string
speeches = []
for speech in speeches_df_filtered_with_biden["speech"]:
  speeches.append(" ".join(word for word in speech))

# Generate matrix of token counts
count_vec = CountVectorizer(lowercase=True, stop_words=None, ngram_range=(1,1))
doc_term_matrix = count_vec.fit_transform(speeches)
print(f"Number of documents: {doc_term_matrix.shape[0]}, Number of words {doc_term_matrix.shape[1]}")
print(f"Previewing first document:\n {doc_term_matrix[0]}")

# Build vocabulary
reverse_vocab_index = {j: i for i,j in count_vec.vocabulary_.items()} # Map to index:word

# Evaluate LDA model performance using topic coherence and determine optimal number of topics
# See performance benchmarks here: https://stackoverflow.com/questions/54762690/what-is-the-meaning-of-coherence-score-0-4-is-it-good-or-bad
# See example here: https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/
# How to determine optimal K: https://stackoverflow.com/questions/62032372/coherence-score-u-mass-18-is-good-or-bad

num_topics = range(1,21)
coherence_vals = []

for i in num_topics:
  # Train LDA model and fit
  LDA = LatentDirichletAllocation(n_components=i, random_state=100)
  LDA.fit(doc_term_matrix)
  doc_topic_distribution = LDA.transform(doc_term_matrix)

  # Determine average coherence
  coherence_val = metric_coherence_gensim(
      'u_mass',
      top_n=20,
      topic_word_distrib=LDA.components_,
      vocab=np.array(list(reverse_vocab_index.values())),
      dtm=doc_term_matrix,
      return_mean=True,
      # texts=speeches
    )
  coherence_vals.append(coherence_val)
  print(f"for {i} topic(s), coherence value: {coherence_val}")

# Plot coherence values
limit=21; start=1; step=1;
x = range(start, limit, step)
plt.plot(x, coherence_vals)
plt.xlabel("Num Topics")
plt.xticks(list(range(1,21)))
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()

OPTIMAL_NUM_TOPICS = 5

# See words in optimal number of topics
LDA = LatentDirichletAllocation(n_components=OPTIMAL_NUM_TOPICS, random_state=100)
LDA.fit(doc_term_matrix)
doc_topic_distribution = LDA.transform(doc_term_matrix)

# Analyze words in each topic
for i, topic in enumerate(LDA.components_):
  print(f"TOPIC {i+1}\n")
  top_words = [idx for idx in reversed(np.argsort(topic))]
  for word_index in top_words[:10]:
    print('%.02f\t%d = %s'%(topic[word_index], word_index, reverse_vocab_index[word_index]))

# Plot each document (speech) in terms of distribution of topics
for i in range(0,speeches_df_filtered.shape[0]):
  plt.bar(np.arange(len(doc_topic_distribution[i]) + 1)[1:],
        doc_topic_distribution[i])
  plt.xlabel("Topics")
  plt.xticks(np.arange(len(doc_topic_distribution[i]) + 1)[1:])
  plt.title(f"President {speeches_df_filtered.iloc[i]['president']} in year {speeches_df_filtered.iloc[i]['year']}")
  plt.show()

"""### Partisanship
[Does the average proportion of health-related words differ between Republicans and Democrats?]  
"""

# Calculate proportions for each party for each president
republican_props_dict, democrat_props_dict = defaultdict(list), defaultdict(list)
for index, row in merged_df.iterrows():
  freq = 0
  for w in row['speech']:
    if w in health_dic:
      freq += 1
  
  prop = freq/len(row['speech'])
  democrat_props_dict[row['president']].append(prop) if row['is_democrat']=='True' else republican_props_dict[row['president']].append(prop)

# Average proportions for each president
republican_props, democrat_props = [], []
for president,props in republican_props_dict.items():
  avg_props = sum(props) / len(props)
  republican_props.append(avg_props)

for president,props in democrat_props_dict.items():
  avg_props = sum(props) / len(props)
  democrat_props.append(avg_props)

print(republican_props)
print(democrat_props)

"""With sample sizes of 4 and 3 for republicans and democrats (respectively), there will be low statistical power and the normality assumption cannot be fulfilled. Moreover, speeches given by a certain president are not independent. Instead, we will run Friedman's test (see assumptions below). See [this document](https://www.statstutor.ac.uk/resources/uploaded/repeated-measures.pdf) for additional reference.  
  
  
This means that the question now shifts to the following...  
**[Is there a difference in the distributions of the proportion of health-reated words (for republicans and democrats) between a president's 1st, 2nd, 3rd, and 4th SOTUA?]**
"""

"""
ASSUMPTIONS:
1. Data is continuous
2. Data comes from a single group* (measured on at least 3 different occasions)
3. Group is random sample from population**
4. Observations are ranked within blocks with no ties

*In this case, the group is the SOTUA order (1st year, 2nd year, 3rd year, 4th year)
**Due to a lack of data, the group is representative of the population (there are only 4 speeches in each group) 

It should also be noted that a president serving two terms are treated as two distinct groups.
"""

# Recreate props dict for republicans and democrats but with strict group (speech) size of 4
"""
NOTE:
- data from Jimmy Carter's speeches are only available for his 2nd, 3rd, 4th, and end of 4th addresses -- we treat this as 1st, 2nd. 3rd 4th
- Jimmy Carter also has speech data from 1978 - 1980 but we skip these as this would form a group with 4 blocks
"""
seen_carter_years = []

# Sort merged_df to guarantee that we can separate a president's 1st and 2nd terms
sorted_merged_df = merged_df.sort_values(by='year')

republican_props_dict, democrat_props_dict = defaultdict(list), defaultdict(list)
for index, row in sorted_merged_df.iterrows():
  if row['president'] == 'jimmy carter':
    if row['year'] in seen_carter_years:
      # Duplicate Carter speech
      continue
    else:
      seen_carter_years.append(row['year'])


  freq = 0
  for w in row['speech']:
    if w in health_dic:
      freq += 1
  prop = freq/len(row['speech'])

  if len(democrat_props_dict[row['president']]) == 4 and row['is_democrat'] == 'True':
    # President is democrat and first term is already filled (with 4 speeches)
    democrat_props_dict[row['president']+'_2'].append(prop)
  elif len(republican_props_dict[row['president']]) == 4 and row['is_democrat']=='False':
    # President is republican and first term is already filled (with 4 speeches)
    republican_props_dict[row['president']+'_2'].append(prop)
  else:
    democrat_props_dict[row['president']].append(prop) if row['is_democrat']=='True' else republican_props_dict[row['president']].append(prop)

# Filter out empty lists in both dicts
clean_republican_props_dict, clean_democrat_props_dict = {}, {}
for k,v in republican_props_dict.items():
  if len(v) != 0:
    clean_republican_props_dict[k] = v
for k,v in democrat_props_dict.items():
  if len(v) != 0:
    clean_democrat_props_dict[k] = v

# print(clean_republican_props_dict)
# print(clean_democrat_props_dict)

# Run Friedman's test on Republicans and Democrats
alpha = 0.05
rep_stat, rep_p = friedmanchisquare(*clean_republican_props_dict.values())
dem_stat, dem_p = friedmanchisquare(*clean_democrat_props_dict.values())

print(rep_p)
print(dem_p)

if rep_p < alpha:
  print("We reject the null that there is no difference between the population distributions of the year a Republican president gave a SOTUA.")
else:
  print("We fail to reject the null that there is no difference between the population distributions of the year a Republican president gave a SOTUA.")

if dem_p < alpha:
  print("We reject the null that there is no difference between the population distributions of the year a Democrat president gave a SOTUA.")
else:
  print("We fail to reject the null that there is no difference between the population distributions of the year a Democrat president gave a SOTUA.")

"""### Frequency of keywords
[Is there a relationship between frequency of health-related words and percentage spend on the health department?]

Loading the health related words dictionary

Finds the frequency of health_dic words per year
"""

def create_data_df(raw_set):
  frequencies = []
  for speech in raw_set["speech"]:
    search_speech = ' '.join(speech)
    freq = 0
    i = 0
    while i < len(search_speech):
      for word in health_dic:
        segment = search_speech[i: i + len(word)]
        if segment == word:
          freq += 1
          i += len(word) - 1
          continue
      i += 1
    frequencies.append(freq / len(speech))
  data = raw_set.copy()
  data['freq'] = frequencies
  
  # Fixes datatype issues
  data['freq'] = pd.to_numeric(data['freq'])
  data['year'] = pd.to_numeric(data['year'])
  data['% spent'] = pd.to_numeric(data['percent health budget spend'])
  data['is_dem'] = data["is_democrat"].apply(lambda x: x == "True")

  data.drop(['url', 'speech', 'percent health budget spend', "is_democrat"], 
            axis=1, inplace=True)
  return data

test_data = create_data_df(test_set)
train_data = create_data_df(train_set)
all_data = test_data.append(train_data)
all_data

"""####Exploratory Graphs"""

#donkey image

donkey = parse_path("""M1275.96,12643.92c-25-28-21-69,17-147
	c19-40,41-106,49-155c20-107,81-231,177-358c38-51,73-98,77-105c3-6-49,28-118,75c-181,124-410,268-519,326c-84,44-110,53-219,71
	c-253,43-519,16-613-60c-15-13-15-19,3-89c42-165,157-357,303-508c125-129,234-199,362-232c32-8,57-26,103-75c82-88,88-100,57-120
	c-13-9-96-47-183-84c-88-38-184-82-214-97l-54-28l73-12c69-12,102-26,102-44c0-19-63-66-183-136c-192-112-291-180-337-230
	c-42-46-52-71-22-59c34,14,169,18,176,6c5-7-19-40-61-84c-38-40-85-100-105-133c-101-166-126-383-78-674c11-65,77-373,86-399
	c5-16,29,27,58,104c63,167,216,414,325,526c63,64,152,129,177,129c8,0,11-119,9-472c-2-500-7-571-55-746c-60-218-186-398-361-514
	c-94-62-199-198-252-324c-104-247-64-528,128-909c186-368,269-471,449-561c84-41,105-48,174-52l79-4l59,73c71,87,93,96,202,80
	c103-16,145-14,208,9c99,34,152,84,262,247c120,177,143,208,230,297c150,154,319,238,560,279c62,11,91,22,118,43
	c77,62,160,115,213,135c277,105,495-92,598-541c14-63,17-133,18-425c1-282,4-371,19-459c63-389,186-755,408-1216
	c107-220,116-262,125-540l5-185l-87-210c-47-115-124-288-171-384l-84-174l25-81c53-175,59-218,59-446c0-195-2-226-27-340
	c-30-143-90-334-145-465l-38-90l-100-1c-163-2-272-50-394-173c-168-170-206-372-99-527c88-129,383-244,738-289l54-7l-59-31
	c-169-90-284-283-284-480c0-234,157-430,395-492c31-8,95-17,143-20c48-4,155-15,237-26c617-80,995,19,1144,300l33,63l-21,72
	c-39,133-74,320-72,383c2,78-11,162-45,280c-25,88-25,95-10,118c21,33,60,33,108,0c20-14,42-25,48-25c21,0,50,83,50,146
	c0,110-54,239-175,414c-125,181-206,374-205,489c1,91,28,204,68,281c21,41,68,131,103,200c74,143,89,196,89,319c0,111-21,193-99,389
	c-83,207-101,277-101,387c0,71,6,105,26,160c14,39,32,95,39,125c55,222,178,322,396,321c129,0,250-26,542-117c133-41,289-83,347-94
	c294-54,661-85,885-75c552,24,1030,128,1715,374c47,17,98,31,114,31h29l-7-223c-12-363-52-640-113-773c-52-114-53-214-8-574
	c26-211,14-345-41-459c-30-59-121-155-177-185c-30-17-45-40-106-165c-39-80-88-193-110-251l-40-105l-75-7c-138-13-234-52-323-132
	c-172-155-205-409-75-574c49-62,133-113,255-155c326-112,372-122,577-121c110,1,162,6,217,21l71,19l-27-40c-128-187-94-454,78-627
	c111-111,219-154,428-169c70-5,180-17,243-26c77-11,185-17,325-17c183-1,223,2,315,22c236,51,386,153,470,320l22,44l-20,71
	c-12,39-28,109-37,156s-23,108-31,137c-14,51-14,52,16,99c37,60,75,174,95,288c8,49,18,92,21,95s31,6,63,6h56l3,39
	c4,48-23,126-91,261c-199,398-192,617,25,835c170,170,307,553,308,856c0,198-62,459-151,636c-28,55-29,64-29,190
	c0,339,88,598,389,1147c216,394,241,450,295,669c27,105,52,192,56,192c12,0,20-55,20-130c0-37,8-105,17-151c22-109,21-298-2-489
	c-10-80-18-210-18-290c-1-125,2-154,22-209l22-63l-20-44c-12-24-30-76-40-116c-20-73-24-97-61-368c-23-167-49-271-89-352l-29-56
	l24-234c62-616,137-928,268-1112c25-36,55-68,66-71c28-9,122,20,205,63c87,46,187,144,243,238c103,174,158,389,198,764
	c21,200,25,988,5,1090c-17,90-39,154-63,185c-17,21-19,36-15,78c9,85-12,258-58,497c-24,124-49,268-55,320s-16,127-22,165
	c-5,39-14,334-18,660c-7,533-10,601-28,701c-26,136-73,286-119,379c-76,152-263,396-433,566c-371,369-790,580-1255,629
	c-282,29-332,32-525,27c-331-9-537-41-1245-192c-521-111-705-136-1220-166c-610-35-1250,102-1829,393c-61,30-113,61-117,69
	s-14,39-23,69c-20,72-92,179-211,315c-53,60-94,116-93,124c2,8,24,20,51,27c52,13,77,29,77,49c0,21-132,140-360,324
	c-118,95-263,216-322,269c-125,110-131,125-67,171l40,30l-38,28c-22,15-126,75-232,133s-221,129-256,158c-86,71-155,164-155,208
	c0,37,14,49,77,62l38,8l-74,48c-40,27-137,78-215,114c-262,120-353,185-434,307c-43,65-41,89,9,136l42,39l-46,19
	c-111,48-497,164-764,231c-35,9-51,23-103,88c-228,286-656,686-791,740c-116,46-332,87-559,107
	C1301.96,12662.92,1290.96,12660.92,1275.96,12643.92z M10113.96,3549.92c21-24,52-72,69-108c29-60,31-73,31-170
	c-1-131-19-200-131-491c-99-256-131-362-146-485c-19-148,11-287,85-400l34-51l-29-79c-63-175-198-418-346-626l-77-108l-35,8
	c-45,9-104,9-158-1l-42-7l34,44c19,25,40,62,47,83c19,54,17,168-4,230c-18,51-18,52,3,92c11,23,23,62,27,88c4,31,21,71,50,116
	c64,98,120,230,211,502c84,252,121,343,176,434c33,55,34,59,39,201c7,151,27,334,53,480c30,169,57,290,65,290
	C10073.96,3591.92,10093.96,3572.92,10113.96,3549.92z""")
donkey.vertices -= donkey.vertices.mean(axis=0)

#elephant image

elephant = parse_path("""M740.71,757.56c26.49-0.12,57.72-12.66,87.41-36.98
	c19.75-16.17,30.72-20.11,58.04-20.83c70.3-1.85,151.69-61.66,189.92-139.57c26.4-53.8,31.32-72.77,50.93-196.43
	c18.75-118.22,31.08-163.71,48.69-179.54c17.09-15.37,31.25-6.67,61.78,37.98c30.3,44.3,48.13,56.43,83.9,57.09
	c47.54,0.87,75.15-28.29,65.12-68.79c-3.43-13.84-8.02-16.06-37.07-17.86l-33.15-2.06l-21.64-41.21
	c-31.49-59.97-46.32-78.71-70.32-88.82c-70.31-29.64-147.22,10.62-190.01,99.47c-20.89,43.39-37.24,58.51-63.23,58.5
	c-6.04,0-22.62,7.11-36.84,15.81c-37.66,23.03-80.47,18.58-80.75-8.38c-0.06-6.08,19.67-49.53,43.85-96.56l43.97-85.49l0.42-78.84
	c0.34-64.07,2.66-82.98,12.37-100.91c11.52-21.28,11.53-22.38,0.31-30.64c-15.41-11.34-122.55-11.77-144.65-0.57
	c-15.93,8.07-16.12,9.12-14.05,76.13c1.82,58.83,0.3,71.45-11.32,94.04c-18.4,35.77-29.04,38.13-41.93,9.27
	c-5.81-13-32.58-51.08-59.48-84.63c-47.07-58.68-48.78-62-45.15-87.81c2.32-16.5,0.9-29.72-3.69-34.33
	c-13.74-13.83-76.16-8.95-114.58,8.96c-32.4,15.11-40.18,16.18-88.42,12.24c-47.84-3.91-54.15-3.07-66.14,8.84
	c-14.86,14.77-15.4,18.81-7.21,54.63c5.1,22.34,2.07,35.28-25.62,109.34c-17.27,46.2-34.37,84.98-37.98,86.17
	c-10.84,3.57-32.16-24.23-42.92-55.97c-18.39-54.23-19.17-97.47-2.7-149.99c16.53-52.7,17.67-69.5,5.28-77.42
	c-11.82-7.55-113.36-7.11-141.53,0.62c-44.1,12.1-48.2,33.67-25.05,131.95c10.7,45.45,12.7,68.02,9.21,104
	c-3.81,39.32-2.01,53.89,12.47,100.87c9.31,30.2,15.95,62.95,14.75,72.78c-2.11,17.3-2.85,16.83-23.05-14.7
	c-11.48-17.92-25.17-34.24-30.41-36.27c-6.7-2.6-8.4-9.24-5.73-22.38c2.3-11.29-0.22-26.52-6.34-38.46
	C47.5,80.03,11.21,50.01,4.63,56.55c-2.12,2.11-4.18,16.7-4.57,32.42c-0.55,21.92,2.92,32.48,14.87,45.28
	c8.57,9.18,19.19,16.71,23.59,16.72c10.85,0.04,45.44,44.91,58.12,75.41c8.1,19.47,9.42,37.24,6.18,82.83
	C96.1,403.57,117.8,472.01,178,546.4c36.15,44.68,110.46,95.66,169.02,115.98c49.02,17,112.17,19.44,257.98,9.96l78.54-5.11
	l-3.43,25.1c-2.64,19.33-0.27,29.14,10.31,42.65C702.35,750.23,720.1,757.66,740.71,757.56L740.71,757.56z M753.45,726.76
	c-10.6,0.01-20.23-3.38-29.8-10.13c-14.09-9.94-14.86-13.63-11.05-53.06c3.73-38.63,2.61-44.54-12.7-66.79
	c-19.78-28.75-27.57-64.83-20.11-93.15c6.72-25.49,42.71-57.39,74.63-66.14c13.41-3.68,30.35-6.69,37.63-6.68
	c16.67,0.01,80.18,31.52,80.15,39.77c-0.01,3.38-12.59-1.27-27.95-10.34c-48.23-28.48-106.94-19.58-141.71,21.46
	c-23.19,27.38-22.62,54.02,2.17,101.33c17.87,34.09,19.39,41.5,15.51,75.38c-5.15,44.86,1.48,55.95,33.5,56.05
	c28.35,0.09,59.91-18.9,79.94-48.09c13.83-20.17,25.1-28.67,25.07-18.91c-0.03,10.25-42.53,53.99-64.21,66.08
	C778.71,722.36,765.46,726.75,753.45,726.76L753.45,726.76z M485.55,76.97c-11.08-0.08-18.43-0.47-18.42-1.3
	c0.01-1.8,7.6-28.06,16.88-58.37c9.28-30.31,19.09-57.42,21.81-60.25c2.71-2.83,22.91,21.46,44.87,53.97
	c21.97,32.52,39,59.95,37.86,60.97C585.65,74.55,518.79,77.2,485.55,76.97L485.55,76.97z""")
elephant.vertices -= elephant.vertices.mean(axis=0)

# General Plotting function

# chages color of scatter points based on president 

COLORS = {
      "donald j. trump": (0.95, 0.9, 0.25),
      "barack obama": (0.4, 0.8, 0.5),
      "george w. bush": (0.9, 0.6, 0),
      "william j. clinton": (0.35, 0.7, 0.9),
      "george bush": (0.8, 0.4, 0),
      "ronald reagan": (0.8, 0.6, 0.7),
      "jimmy carter": (0, 0.45, 0.7)
  }

def get_colors(row):
  return COLORS[row['president']]

def get_graphable(x_var, y_var):
  graph_data = pd.DataFrame()
  graph_data[x_var.title()] = all_data[x_var]
  graph_data[y_var.title()] = all_data[y_var]
  graph_data["Political Party"] = all_data["is_dem"].replace(
      {True: "Democrat", False: "Republican"})
  graph_data["President"] = all_data["president"].map(lambda s: s.title())

  graph_colors = {k.title(): v for k, v in COLORS.items()}
  return (graph_data, graph_colors)

# changes marker symbol based on partisanship
def get_marker(row):
  return "*" if row["is_dem"] else "p"

marker_dict = {"Democrat": donkey, "Republican": elephant}

def scale_axis(data_label):
  SCALING = 1.08
  range = (max(all_data[data_label]) - min(all_data[data_label]))*SCALING
  avg = (max(all_data[data_label]) + min(all_data[data_label])) / 2
  return (avg - range/2, avg + range/2)
  
def plot_items(x_var, y_var, ax):
  ax.set(ylim=scale_axis(y_var), xlim=scale_axis(x_var))

  graph_data = get_graphable(x_var, y_var)[0]
  graph_colors = get_graphable(x_var, y_var)[1]

  sns.scatterplot(data=graph_data, x=x_var.title(), y=y_var.title(), 
                  hue='President', palette=graph_colors, ax=ax,
                  style='Political Party', s=200, alpha=0.7, markers = marker_dict, edgecolor = "black", linewidth = 0.6)

# Change in healthcare mentions over time
fig, ax = plt.subplots()

plot_items('year', 'freq', ax)
ax.set_xlabel("Year")
ax.set_title("Frequency of Health-Related Words in SOTUA")
ax.set_xticks(range(min(all_data["year"]), max(all_data["year"]) + 1, 4))

# brief linear regression to show general pattern
m, b = np.polyfit(all_data["year"], all_data["freq"], 1)
ax.plot(all_data["year"], m*all_data["year"]+b, color='black',
        label='Standard regression line')
legend = ax.legend(bbox_to_anchor=(1.5, 1))
for handle in legend.Handles:
  handle._sizes = [30]

# Healthcare mentions compared by partisanship

def partisan_swarm(y_var, ax):
  graph_data = get_graphable('is_dem', y_var)[0]
  graph_colors = get_graphable('is_dem', y_var)[1]

  sns.swarmplot(x='Political Party', y=y_var.title(), data=graph_data, 
                hue='President', palette=graph_colors, s=7.5,
                ax=ax)

fig, ax = plt.subplots()

partisan_swarm('freq', ax)
ax.set_title("Frequency of Health-Related Words in SOTUA")
ax.legend(bbox_to_anchor=(1.5, 1))

# graph frequencies of healthcare terms vs year
fig, ax = plt.subplots()
plot_items('year', '% spent', ax)
ax.set_xlabel("Year")
ax.set_title("Percentage of Anual Budget Spent on HHS")
ax.set_xticks(range(min(all_data["year"]), max(all_data["year"]) + 1, 4))

# brief linear regression to show general pattern
m, b = np.polyfit(all_data["year"], all_data["% spent"], 1)
# ax.plot(all_data["year"], m*all_data["year"]+b, color='black',
#         label='Standard regression line')
ax.legend(bbox_to_anchor=(1.1, 1))

# compares partisanship and healthcare spending
fig, ax = plt.subplots()

partisan_swarm('% spent', ax)
ax.set_title("Percentage of Anual Budget Spent on HHS")
ax.legend(bbox_to_anchor=(1.5, 1))

"""#### Standard regression

"""

# Plots all the data points on a healthcare word frequency vs spending plot
def scatter_plot():
  fig, ax = plt.subplots()

  # plots percent spent and percent spoken on health

  plot_items("freq", "% spent", ax)

  ax.set_title("Percentage of time spoken on health issues vs percentage of budget spent")
  ax.set_ylabel("Percentage spent on health")
  ax.set_xlabel("Percentage spoken on health")

  ax.legend(bbox_to_anchor=(1.1, 1))

  return fig, ax

scatter_plot()

# linear regression

def run_regression(train_data, test_data):
  X = sm.add_constant(train_data['freq'])
  y = train_data['% spent']

  model = sm.OLS(y, X).fit()

  print(model.summary())
  print(f" - MSE Training:{eval_measures.mse(y, model.predict(X))}")

  def get_x():
    if len(test_data['freq']) > 1:
      return sm.add_constant(test_data['freq'])
    else:
      test_data['const'] = [1]
      return test_data[['const', 'freq']]
  if len(test_data) != 0:
    mse_test = eval_measures.mse(test_data['% spent'], 
                               model.predict(get_x()))
  
    print(f" - MSE Testing: {mse_test}")
  return model

fig, ax = scatter_plot()

def plot_line(model, name=None, color='black', ax=ax):
  x_space = np.linspace(all_data['freq'].min(),all_data['freq'].max(),300)
  ax.plot(x_space, model.predict(sm.add_constant(x_space)),
        color=color, label=name)
  
model = run_regression(train_data, test_data)
plot_line(model, 'Standard regression line')
ax.legend(bbox_to_anchor=(1.1, 1))

"""Noticibly, the data divides into clusters before and after clinton. Thus, we decided to run regression by cluster to avoid simpsons paradox. We give the following justification
1. Clinton's Health Security Act healthcare reform push in 1993 most likely introduced tons of healthcare vocabulary into mainstream discourse, whereas there was never that large of an emphasis on healthcare before him. Many words from our dictionary were most likely unwittingly influenced by Clinton.
2. There was a sizable spike in the 90s in healthcare spending
3. Health care prices accelerated in the 90s, becoming double the rate of inflation.

#### Pre - post Clinton cluster Split regression
"""

# Splits

POST_CLINTON_PREZ = ["donald j. trump", "barack obama", "george w. bush", "william j. clinton"]

pre_clinton_test = test_data[~test_data['president'].isin(POST_CLINTON_PREZ)]
post_clinton_test = test_data[test_data['president'].isin(POST_CLINTON_PREZ)]

pre_clinton_train = train_data[~train_data['president'].isin(POST_CLINTON_PREZ)]
post_clinton_train = train_data[train_data['president'].isin(POST_CLINTON_PREZ)]

print(pre_clinton_test.shape)
print(pre_clinton_train.shape)
print(post_clinton_test.shape)
print(post_clinton_train.shape)

print("=== PRE CLINTON ===")
model = run_regression(pre_clinton_train, pre_clinton_test)

fig, ax = scatter_plot()
plot_line(model, 'Pre-Clinton regression', ax=ax)
ax.legend(bbox_to_anchor=(1.1, 1))

print("=== POST CLINTON ===")
model = run_regression(post_clinton_train, post_clinton_test)

fig, ax = scatter_plot()
plot_line(model, 'Post-Clinton Regression', ax=ax)
ax.legend(bbox_to_anchor=(1.1, 1))

"""#### Split by each president"""

all_fig, all_ax = scatter_plot()
all_ax.set_title("Regression by each president")

prez_coeff = {"president" : [], "coeff": []}

def presidential_regression(name):
  # datasets too small to get a train and test set
  train_prez = pd.concat([train_data[train_data['president'] == name],
                         test_data[test_data['president'] == name]])

  model = run_regression(train_prez, [])

  plot_line(model, name=None, color=COLORS[name], ax=all_ax)
  prez_coeff["president"].append(name)
  prez_coeff["coeff"].append(model.params[1])
  
  fig_prez, ax_prez = scatter_plot()
  plot_line(model, name=None, color=COLORS[name], ax=ax_prez)
  ax_prez.set_title(f"Regression - {name}")
  fig_prez.show()

#Jimmy Carter Regression
presidential_regression("jimmy carter")

presidential_regression("ronald reagan")

presidential_regression("george bush")

presidential_regression("william j. clinton")

presidential_regression("george w. bush")

presidential_regression("barack obama")

presidential_regression("donald j. trump")

all_fig

plt.plot(prez_coeff["president"], prez_coeff["coeff"], "o")
plt.xticks(rotation=45)
plt.title("Regression Coefficients for Presidents since Carter")
plt.xlabel("President")
plt.ylabel("Coefficient")